{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "random_seed = 77\n",
    "np.random.seed(random_seed)\n",
    "tf.set_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "train_data_path = \"./data/train_data.npy\"\n",
    "train_label_path = \"./data/train_label.npy\"\n",
    "test_data_path = \"./data/test_data.npy\"\n",
    "test_label_path = \"./data/test_label.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "test_size = 0.1\n",
    "lr = 0.0002\n",
    "batch_size = 100\n",
    "epochs = 300\n",
    "keep_probab = 1.0\n",
    "image_shape = (32, 32, 3)\n",
    "n_classes = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading data\n",
    "data_features = np.load(train_data_path)\n",
    "data_labels = np.load(train_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# normalizing data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data_features)\n",
    "data_features = scaler.transform(data_features)\n",
    "data_features = np.reshape(data_features, (len(data_features), 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dividing data into train and validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data_features, data_labels, test_size=test_size, stratify=data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 52340\n",
      "Number of test examples: 5816\n",
      "Image shape: (32, 32, 3)\n",
      "Number of unique classes: 43\n"
     ]
    }
   ],
   "source": [
    "# status of data after various different image augmentation operations\n",
    "# like rotation by 45 degree, image noising, gaussian blurring\n",
    "# addition of cropped image\n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_val.shape[0]\n",
    "print(\"Number of training examples: \" + str(n_train))\n",
    "print(\"Number of test examples: \" + str(n_test))\n",
    "print(\"Image shape: \" + str(image_shape))\n",
    "print(\"Number of unique classes: \" + str(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npwwASeqUASBJnTIAJKlT/x8zlLaQXxkOkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f230d169610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for checking number of images in each class of train data\n",
    "plt.hist(Y_train, bins=range(44), color='#00ffCC')\n",
    "plt.title(\"Number of images in each class of train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHXORC9JnTPRS1Ln/j+jgbnuKEFDPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f23024a0ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot for checking number of images in each class of train data\n",
    "plt.hist(Y_val, bins=range(44), color='#00ffCC')\n",
    "plt.title(\"Number of images in each class of validation data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting labels into one_hot_vector\n",
    "Y_train_one_hot = pd.get_dummies(pd.Series(Y_train))\n",
    "Y_val_one_hot = pd.get_dummies(pd.Series(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight function\n",
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bias function\n",
    "def bias_variable(shape):\n",
    "    init = tf.truncated_normal(shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolution function\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(input=x, filter=W, strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max-pooling functon\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Network Architecture\n",
    "# input layer\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "Y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# first convolutional layer\n",
    "w_conv1 = weight_variable([3, 3, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(X, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool(h_conv2)\n",
    "\n",
    "# third convolutional layer\n",
    "w_conv3 = weight_variable([3, 3, 64, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, w_conv3) + b_conv3)\n",
    "h_pool3 = max_pool(h_conv3)\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 4 * 4 * 128])\n",
    "\n",
    "# fully connected layer\n",
    "w_fc1 = weight_variable([4 * 4 * 128, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout layer\n",
    "h_fc1_dropout = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# output layer\n",
    "w_fc2 = weight_variable([1024, n_classes])\n",
    "b_fc2 = bias_variable([n_classes])\n",
    "out = tf.add(tf.matmul(h_fc1_dropout, w_fc2), b_fc2)\n",
    "\n",
    "# getting predictions\n",
    "pred_step = tf.argmax(out, 1)\n",
    "\n",
    "# cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "# evaluation step\n",
    "matches = tf.equal(tf.argmax(out, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "\n",
    "# initializing session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  Epoch=0  TrainLoss=172364.930385  TrainAcc=0.169373328238\n",
      "Valid:  Epoch=0  ValidLoss=157166.0  ValAcc=0.165922\n",
      "\n",
      "Train:  Epoch=1  TrainLoss=100464.035722  TrainAcc=0.273232709209\n",
      "Valid:  Epoch=1  ValidLoss=93755.4  ValAcc=0.268741\n",
      "\n",
      "Train:  Epoch=2  TrainLoss=69030.6414716  TrainAcc=0.351910584639\n",
      "Valid:  Epoch=2  ValidLoss=65482.9  ValAcc=0.347318\n",
      "\n",
      "Train:  Epoch=3  TrainLoss=51410.9547406  TrainAcc=0.41079480321\n",
      "Valid:  Epoch=3  ValidLoss=49953.0  ValAcc=0.399587\n",
      "\n",
      "Train:  Epoch=4  TrainLoss=40377.9805514  TrainAcc=0.462399694306\n",
      "Valid:  Epoch=4  ValidLoss=40112.2  ValAcc=0.44945\n",
      "\n",
      "Train:  Epoch=5  TrainLoss=32528.1424048  TrainAcc=0.503725640046\n",
      "Valid:  Epoch=5  ValidLoss=33279.7  ValAcc=0.484869\n",
      "\n",
      "Train:  Epoch=6  TrainLoss=26693.6726774  TrainAcc=0.547688192587\n",
      "Valid:  Epoch=6  ValidLoss=28256.0  ValAcc=0.524072\n",
      "\n",
      "Train:  Epoch=7  TrainLoss=22192.3538713  TrainAcc=0.583683607184\n",
      "Valid:  Epoch=7  ValidLoss=24423.4  ValAcc=0.548315\n",
      "\n",
      "Train:  Epoch=8  TrainLoss=18622.6439084  TrainAcc=0.618379824226\n",
      "Valid:  Epoch=8  ValidLoss=21259.7  ValAcc=0.576685\n",
      "\n",
      "Train:  Epoch=9  TrainLoss=15967.3638499  TrainAcc=0.646809323653\n",
      "Valid:  Epoch=9  ValidLoss=18991.5  ValAcc=0.600756\n",
      "\n",
      "Train:  Epoch=10  TrainLoss=13388.4391273  TrainAcc=0.673767672908\n",
      "Valid:  Epoch=10  ValidLoss=16880.2  ValAcc=0.622077\n",
      "\n",
      "Train:  Epoch=11  TrainLoss=11512.466777  TrainAcc=0.699197554452\n",
      "Valid:  Epoch=11  ValidLoss=15184.6  ValAcc=0.647008\n",
      "\n",
      "Train:  Epoch=12  TrainLoss=9820.77728091  TrainAcc=0.723710355369\n",
      "Valid:  Epoch=12  ValidLoss=13826.2  ValAcc=0.66317\n",
      "\n",
      "Train:  Epoch=13  TrainLoss=8541.32528741  TrainAcc=0.743752388231\n",
      "Valid:  Epoch=13  ValidLoss=12648.4  ValAcc=0.680021\n",
      "\n",
      "Train:  Epoch=14  TrainLoss=7305.11694136  TrainAcc=0.764348490638\n",
      "Valid:  Epoch=14  ValidLoss=11594.8  ValAcc=0.694807\n",
      "\n",
      "Train:  Epoch=15  TrainLoss=6474.14100533  TrainAcc=0.779117309897\n",
      "Valid:  Epoch=15  ValidLoss=10858.6  ValAcc=0.710282\n",
      "\n",
      "Train:  Epoch=16  TrainLoss=5323.03757859  TrainAcc=0.801528467711\n",
      "Valid:  Epoch=16  ValidLoss=9832.2  ValAcc=0.723693\n",
      "\n",
      "Train:  Epoch=17  TrainLoss=4588.16829777  TrainAcc=0.814673290027\n",
      "Valid:  Epoch=17  ValidLoss=9153.46  ValAcc=0.736073\n",
      "\n",
      "Train:  Epoch=18  TrainLoss=3979.62933594  TrainAcc=0.827397783722\n",
      "Valid:  Epoch=18  ValidLoss=8695.67  ValAcc=0.748452\n",
      "\n",
      "Train:  Epoch=19  TrainLoss=3427.53210588  TrainAcc=0.84105846389\n",
      "Valid:  Epoch=19  ValidLoss=8162.19  ValAcc=0.759457\n",
      "\n",
      "Train:  Epoch=20  TrainLoss=3038.25847563  TrainAcc=0.85047764616\n",
      "Valid:  Epoch=20  ValidLoss=7824.78  ValAcc=0.768741\n",
      "\n",
      "Train:  Epoch=21  TrainLoss=2568.20303746  TrainAcc=0.863928162018\n",
      "Valid:  Epoch=21  ValidLoss=7364.23  ValAcc=0.780433\n",
      "\n",
      "Train:  Epoch=22  TrainLoss=2293.08012522  TrainAcc=0.870596102407\n",
      "Valid:  Epoch=22  ValidLoss=7030.78  ValAcc=0.7837\n",
      "\n",
      "Train:  Epoch=23  TrainLoss=1920.02284919  TrainAcc=0.881734810852\n",
      "Valid:  Epoch=23  ValidLoss=6615.66  ValAcc=0.795048\n",
      "\n",
      "Train:  Epoch=24  TrainLoss=1716.05633484  TrainAcc=0.891765380206\n",
      "Valid:  Epoch=24  ValidLoss=6354.86  ValAcc=0.803129\n",
      "\n",
      "Train:  Epoch=25  TrainLoss=1629.51544696  TrainAcc=0.893886129156\n",
      "Valid:  Epoch=25  ValidLoss=6185.01  ValAcc=0.805708\n",
      "\n",
      "Train:  Epoch=26  TrainLoss=1437.35486137  TrainAcc=0.902158960642\n",
      "Valid:  Epoch=26  ValidLoss=5871.79  ValAcc=0.809663\n",
      "\n",
      "Train:  Epoch=27  TrainLoss=1269.66330873  TrainAcc=0.907069163164\n",
      "Valid:  Epoch=27  ValidLoss=5777.03  ValAcc=0.817056\n",
      "\n",
      "Train:  Epoch=28  TrainLoss=1283.85404973  TrainAcc=0.908865112724\n",
      "Valid:  Epoch=28  ValidLoss=5743.4  ValAcc=0.818432\n",
      "\n",
      "Train:  Epoch=29  TrainLoss=964.008576566  TrainAcc=0.92290790982\n",
      "Valid:  Epoch=29  ValidLoss=5357.37  ValAcc=0.829264\n",
      "\n",
      "Train:  Epoch=30  TrainLoss=874.226521686  TrainAcc=0.92867787543\n",
      "Valid:  Epoch=30  ValidLoss=5110.9  ValAcc=0.836829\n",
      "\n",
      "Train:  Epoch=31  TrainLoss=878.233768963  TrainAcc=0.926538020634\n",
      "Valid:  Epoch=31  ValidLoss=5184.21  ValAcc=0.834594\n",
      "\n",
      "Train:  Epoch=32  TrainLoss=763.637066886  TrainAcc=0.932709209018\n",
      "Valid:  Epoch=32  ValidLoss=4954.79  ValAcc=0.835282\n",
      "\n",
      "Train:  Epoch=33  TrainLoss=656.108976128  TrainAcc=0.93872755063\n",
      "Valid:  Epoch=33  ValidLoss=4902.48  ValAcc=0.843879\n",
      "\n",
      "Train:  Epoch=34  TrainLoss=570.52824294  TrainAcc=0.942472296523\n",
      "Valid:  Epoch=34  ValidLoss=4760.66  ValAcc=0.844738\n",
      "\n",
      "Train:  Epoch=35  TrainLoss=593.363217845  TrainAcc=0.940638135269\n",
      "Valid:  Epoch=35  ValidLoss=4675.35  ValAcc=0.844223\n",
      "\n",
      "Train:  Epoch=36  TrainLoss=570.963963832  TrainAcc=0.943618647306\n",
      "Valid:  Epoch=36  ValidLoss=4806.44  ValAcc=0.84749\n",
      "\n",
      "Train:  Epoch=37  TrainLoss=394.717905819  TrainAcc=0.957909820405\n",
      "Valid:  Epoch=37  ValidLoss=4503.52  ValAcc=0.857978\n",
      "\n",
      "Train:  Epoch=38  TrainLoss=513.669695679  TrainAcc=0.949293083684\n",
      "Valid:  Epoch=38  ValidLoss=4579.47  ValAcc=0.850241\n",
      "\n",
      "Train:  Epoch=39  TrainLoss=419.346001759  TrainAcc=0.958215513947\n",
      "Valid:  Epoch=39  ValidLoss=4455.88  ValAcc=0.855571\n",
      "\n",
      "Train:  Epoch=40  TrainLoss=433.378181648  TrainAcc=0.956037447459\n",
      "Valid:  Epoch=40  ValidLoss=4463.93  ValAcc=0.855399\n",
      "\n",
      "Train:  Epoch=41  TrainLoss=375.097499819  TrainAcc=0.959495605655\n",
      "Valid:  Epoch=41  ValidLoss=4237.92  ValAcc=0.855743\n",
      "\n",
      "Train:  Epoch=42  TrainLoss=295.540518644  TrainAcc=0.965991593428\n",
      "Valid:  Epoch=42  ValidLoss=4223.48  ValAcc=0.865887\n",
      "\n",
      "Train:  Epoch=43  TrainLoss=214.99721281  TrainAcc=0.971857088269\n",
      "Valid:  Epoch=43  ValidLoss=4024.98  ValAcc=0.868294\n",
      "\n",
      "Train:  Epoch=44  TrainLoss=341.309447916  TrainAcc=0.961444401987\n",
      "Valid:  Epoch=44  ValidLoss=4171.21  ValAcc=0.860557\n",
      "\n",
      "Train:  Epoch=45  TrainLoss=309.5176847  TrainAcc=0.963221245701\n",
      "Valid:  Epoch=45  ValidLoss=4072.87  ValAcc=0.86795\n",
      "\n",
      "Train:  Epoch=46  TrainLoss=237.106214846  TrainAcc=0.969984715323\n",
      "Valid:  Epoch=46  ValidLoss=4019.02  ValAcc=0.86967\n",
      "\n",
      "Train:  Epoch=47  TrainLoss=317.582749051  TrainAcc=0.964807030951\n",
      "Valid:  Epoch=47  ValidLoss=4088.47  ValAcc=0.865371\n",
      "\n",
      "Train:  Epoch=48  TrainLoss=223.379089939  TrainAcc=0.972888803974\n",
      "Valid:  Epoch=48  ValidLoss=3841.3  ValAcc=0.87861\n",
      "\n",
      "Train:  Epoch=49  TrainLoss=210.910471339  TrainAcc=0.973557508598\n",
      "Valid:  Epoch=49  ValidLoss=3881.14  ValAcc=0.877751\n",
      "\n",
      "Train:  Epoch=50  TrainLoss=227.51521432  TrainAcc=0.971016431028\n",
      "Valid:  Epoch=50  ValidLoss=3942.26  ValAcc=0.87414\n",
      "\n",
      "Train:  Epoch=51  TrainLoss=164.105481822  TrainAcc=0.977646159725\n",
      "Valid:  Epoch=51  ValidLoss=3720.52  ValAcc=0.881877\n",
      "\n",
      "Train:  Epoch=52  TrainLoss=203.078908888  TrainAcc=0.972774168896\n",
      "Valid:  Epoch=52  ValidLoss=3632.05  ValAcc=0.873968\n",
      "\n",
      "Train:  Epoch=53  TrainLoss=298.804709533  TrainAcc=0.970806266718\n",
      "Valid:  Epoch=53  ValidLoss=3814.3  ValAcc=0.875344\n",
      "\n",
      "Train:  Epoch=54  TrainLoss=165.269156834  TrainAcc=0.979174627436\n",
      "Valid:  Epoch=54  ValidLoss=3688.55  ValAcc=0.881018\n",
      "\n",
      "Train:  Epoch=55  TrainLoss=160.833265893  TrainAcc=0.978391287734\n",
      "Valid:  Epoch=55  ValidLoss=3875.53  ValAcc=0.881705\n",
      "\n",
      "Train:  Epoch=56  TrainLoss=136.051959155  TrainAcc=0.978028276653\n",
      "Valid:  Epoch=56  ValidLoss=3596.54  ValAcc=0.883769\n",
      "\n",
      "Train:  Epoch=57  TrainLoss=129.739954217  TrainAcc=0.982690103172\n",
      "Valid:  Epoch=57  ValidLoss=3574.79  ValAcc=0.890302\n",
      "\n",
      "Train:  Epoch=58  TrainLoss=107.684394664  TrainAcc=0.983683607184\n",
      "Valid:  Epoch=58  ValidLoss=3342.92  ValAcc=0.889099\n",
      "\n",
      "Train:  Epoch=59  TrainLoss=139.090066177  TrainAcc=0.978754298815\n",
      "Valid:  Epoch=59  ValidLoss=3441.18  ValAcc=0.888583\n",
      "\n",
      "Train:  Epoch=60  TrainLoss=133.256200568  TrainAcc=0.980397401605\n",
      "Valid:  Epoch=60  ValidLoss=3402.71  ValAcc=0.887723\n",
      "\n",
      "Train:  Epoch=61  TrainLoss=113.731393948  TrainAcc=0.980225448987\n",
      "Valid:  Epoch=61  ValidLoss=3392.39  ValAcc=0.887723\n",
      "\n",
      "Train:  Epoch=62  TrainLoss=105.594743452  TrainAcc=0.985001910585\n",
      "Valid:  Epoch=62  ValidLoss=3201.71  ValAcc=0.892194\n",
      "\n",
      "Train:  Epoch=63  TrainLoss=140.699830373  TrainAcc=0.981543752388\n",
      "Valid:  Epoch=63  ValidLoss=3361.01  ValAcc=0.893053\n",
      "\n",
      "Train:  Epoch=64  TrainLoss=98.3261955634  TrainAcc=0.982842949943\n",
      "Valid:  Epoch=64  ValidLoss=3167.4  ValAcc=0.895461\n",
      "\n",
      "Train:  Epoch=65  TrainLoss=106.496045576  TrainAcc=0.982460833015\n",
      "Valid:  Epoch=65  ValidLoss=3308.33  ValAcc=0.891506\n",
      "\n",
      "Train:  Epoch=66  TrainLoss=84.3120747713  TrainAcc=0.987485670615\n",
      "Valid:  Epoch=66  ValidLoss=3326.69  ValAcc=0.895633\n",
      "\n",
      "Train:  Epoch=67  TrainLoss=103.862316769  TrainAcc=0.982823844096\n",
      "Valid:  Epoch=67  ValidLoss=3246.24  ValAcc=0.894257\n",
      "\n",
      "Train:  Epoch=68  TrainLoss=104.32111368  TrainAcc=0.987122659534\n",
      "Valid:  Epoch=68  ValidLoss=3102.72  ValAcc=0.898727\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  Epoch=69  TrainLoss=79.3588713492  TrainAcc=0.98528849828\n",
      "Valid:  Epoch=69  ValidLoss=3112.94  ValAcc=0.898555\n",
      "\n",
      "Train:  Epoch=70  TrainLoss=66.817091014  TrainAcc=0.986836071838\n",
      "Valid:  Epoch=70  ValidLoss=2951.05  ValAcc=0.902854\n",
      "\n",
      "Train:  Epoch=71  TrainLoss=73.8737525167  TrainAcc=0.986071837982\n",
      "Valid:  Epoch=71  ValidLoss=2964.48  ValAcc=0.903714\n",
      "\n",
      "Train:  Epoch=72  TrainLoss=110.73771459  TrainAcc=0.982288880397\n",
      "Valid:  Epoch=72  ValidLoss=3084.76  ValAcc=0.897868\n",
      "\n",
      "Train:  Epoch=73  TrainLoss=88.5157681148  TrainAcc=0.985231180741\n",
      "Valid:  Epoch=73  ValidLoss=2948.53  ValAcc=0.898384\n",
      "\n",
      "Train:  Epoch=74  TrainLoss=139.735910718  TrainAcc=0.980320978219\n",
      "Valid:  Epoch=74  ValidLoss=3123.87  ValAcc=0.894773\n",
      "\n",
      "Train:  Epoch=75  TrainLoss=99.0686421903  TrainAcc=0.988632021399\n",
      "Valid:  Epoch=75  ValidLoss=3089.65  ValAcc=0.898727\n",
      "\n",
      "Train:  Epoch=76  TrainLoss=55.0104607156  TrainAcc=0.989358043561\n",
      "Valid:  Epoch=76  ValidLoss=2893.85  ValAcc=0.901994\n",
      "\n",
      "Train:  Epoch=77  TrainLoss=51.1851402924  TrainAcc=0.990982040504\n",
      "Valid:  Epoch=77  ValidLoss=2872.64  ValAcc=0.904229\n",
      "\n",
      "Train:  Epoch=78  TrainLoss=76.6823840419  TrainAcc=0.987447458922\n",
      "Valid:  Epoch=78  ValidLoss=3033.85  ValAcc=0.90165\n",
      "\n",
      "Train:  Epoch=79  TrainLoss=91.2501244574  TrainAcc=0.987160871227\n",
      "Valid:  Epoch=79  ValidLoss=3092.18  ValAcc=0.904917\n",
      "\n",
      "Train:  Epoch=80  TrainLoss=38.7181903266  TrainAcc=0.992032862056\n",
      "Valid:  Epoch=80  ValidLoss=3014.77  ValAcc=0.906465\n",
      "\n",
      "Train:  Epoch=81  TrainLoss=67.5045587775  TrainAcc=0.98920519679\n",
      "Valid:  Epoch=81  ValidLoss=2979.38  ValAcc=0.904917\n",
      "\n",
      "Train:  Epoch=82  TrainLoss=53.6549269499  TrainAcc=0.99243408483\n",
      "Valid:  Epoch=82  ValidLoss=3041.02  ValAcc=0.909904\n",
      "\n",
      "Train:  Epoch=83  TrainLoss=151.649251763  TrainAcc=0.985116545663\n",
      "Valid:  Epoch=83  ValidLoss=3132.57  ValAcc=0.900791\n",
      "\n",
      "Train:  Epoch=84  TrainLoss=46.1413747587  TrainAcc=0.99105846389\n",
      "Valid:  Epoch=84  ValidLoss=2726.83  ValAcc=0.910763\n",
      "\n",
      "Train:  Epoch=85  TrainLoss=90.0233370261  TrainAcc=0.986320213985\n",
      "Valid:  Epoch=85  ValidLoss=2795.8  ValAcc=0.905777\n",
      "\n",
      "Train:  Epoch=86  TrainLoss=79.210230484  TrainAcc=0.988192586932\n",
      "Valid:  Epoch=86  ValidLoss=2915.94  ValAcc=0.908184\n",
      "\n",
      "Train:  Epoch=87  TrainLoss=55.204279314  TrainAcc=0.988918609094\n",
      "Valid:  Epoch=87  ValidLoss=2702.21  ValAcc=0.911967\n",
      "\n",
      "Train:  Epoch=88  TrainLoss=44.7741578017  TrainAcc=0.991822697746\n",
      "Valid:  Epoch=88  ValidLoss=2704.9  ValAcc=0.909904\n",
      "\n",
      "Train:  Epoch=89  TrainLoss=68.0890214196  TrainAcc=0.990752770348\n",
      "Valid:  Epoch=89  ValidLoss=2874.75  ValAcc=0.90784\n",
      "\n",
      "Train:  Epoch=90  TrainLoss=75.9212298757  TrainAcc=0.98666411922\n",
      "Valid:  Epoch=90  ValidLoss=2712.15  ValAcc=0.906465\n",
      "\n",
      "Train:  Epoch=91  TrainLoss=28.2680626952  TrainAcc=0.994631257165\n",
      "Valid:  Epoch=91  ValidLoss=2496.36  ValAcc=0.911623\n",
      "\n",
      "Train:  Epoch=92  TrainLoss=47.9301568795  TrainAcc=0.991994650363\n",
      "Valid:  Epoch=92  ValidLoss=2693.09  ValAcc=0.910763\n",
      "\n",
      "Train:  Epoch=93  TrainLoss=21.3349515121  TrainAcc=0.994822315629\n",
      "Valid:  Epoch=93  ValidLoss=2615.64  ValAcc=0.915749\n",
      "\n",
      "Train:  Epoch=94  TrainLoss=33.3035877471  TrainAcc=0.993484906381\n",
      "Valid:  Epoch=94  ValidLoss=2672.9  ValAcc=0.915406\n",
      "\n",
      "Train:  Epoch=95  TrainLoss=36.0727254485  TrainAcc=0.993465800535\n",
      "Valid:  Epoch=95  ValidLoss=2729.82  ValAcc=0.916781\n",
      "\n",
      "Train:  Epoch=96  TrainLoss=56.2744305113  TrainAcc=0.989778372182\n",
      "Valid:  Epoch=96  ValidLoss=2659.67  ValAcc=0.912826\n",
      "\n",
      "Train:  Epoch=97  TrainLoss=79.4491284736  TrainAcc=0.989186090944\n",
      "Valid:  Epoch=97  ValidLoss=2703.93  ValAcc=0.909044\n",
      "\n",
      "Train:  Epoch=98  TrainLoss=27.2349531316  TrainAcc=0.994134505159\n",
      "Valid:  Epoch=98  ValidLoss=2471.62  ValAcc=0.917641\n",
      "\n",
      "Train:  Epoch=99  TrainLoss=56.942166517  TrainAcc=0.992128391288\n",
      "Valid:  Epoch=99  ValidLoss=2692.27  ValAcc=0.913686\n",
      "\n",
      "Train:  Epoch=100  TrainLoss=76.4019328763  TrainAcc=0.990867405426\n",
      "Valid:  Epoch=100  ValidLoss=2700.27  ValAcc=0.912311\n",
      "\n",
      "Train:  Epoch=101  TrainLoss=59.9515901221  TrainAcc=0.99058081773\n",
      "Valid:  Epoch=101  ValidLoss=2607.44  ValAcc=0.911795\n",
      "\n",
      "Train:  Epoch=102  TrainLoss=75.0695972097  TrainAcc=0.989224302637\n",
      "Valid:  Epoch=102  ValidLoss=2609.49  ValAcc=0.907324\n",
      "\n",
      "Train:  Epoch=103  TrainLoss=59.451006314  TrainAcc=0.993637753152\n",
      "Valid:  Epoch=103  ValidLoss=2530.98  ValAcc=0.91936\n",
      "\n",
      "Train:  Epoch=104  TrainLoss=57.8090945327  TrainAcc=0.992472296523\n",
      "Valid:  Epoch=104  ValidLoss=2497.12  ValAcc=0.916437\n",
      "\n",
      "Train:  Epoch=105  TrainLoss=56.8952133304  TrainAcc=0.992777990065\n",
      "Valid:  Epoch=105  ValidLoss=2528.49  ValAcc=0.9185\n",
      "\n",
      "Train:  Epoch=106  TrainLoss=63.705120298  TrainAcc=0.992338555598\n",
      "Valid:  Epoch=106  ValidLoss=2538.38  ValAcc=0.915921\n",
      "\n",
      "Train:  Epoch=107  TrainLoss=56.6059136163  TrainAcc=0.990217806649\n",
      "Valid:  Epoch=107  ValidLoss=2529.03  ValAcc=0.916265\n",
      "\n",
      "Train:  Epoch=108  TrainLoss=56.2543897393  TrainAcc=0.993542223921\n",
      "Valid:  Epoch=108  ValidLoss=2491.58  ValAcc=0.917813\n",
      "\n",
      "Train:  Epoch=109  TrainLoss=26.1524046513  TrainAcc=0.994784103936\n",
      "Valid:  Epoch=109  ValidLoss=2315.73  ValAcc=0.91936\n",
      "\n",
      "Train:  Epoch=110  TrainLoss=40.0843019966  TrainAcc=0.993370271303\n",
      "Valid:  Epoch=110  ValidLoss=2362.03  ValAcc=0.921767\n",
      "\n",
      "Train:  Epoch=111  TrainLoss=14.0758101677  TrainAcc=0.996599159343\n",
      "Valid:  Epoch=111  ValidLoss=2247.4  ValAcc=0.920048\n",
      "\n",
      "Train:  Epoch=112  TrainLoss=67.7544829325  TrainAcc=0.992663354987\n",
      "Valid:  Epoch=112  ValidLoss=2518.75  ValAcc=0.918844\n",
      "\n",
      "Train:  Epoch=113  TrainLoss=30.7914326718  TrainAcc=0.994153611005\n",
      "Valid:  Epoch=113  ValidLoss=2395.87  ValAcc=0.919532\n",
      "\n",
      "Train:  Epoch=114  TrainLoss=50.9825552737  TrainAcc=0.993389377149\n",
      "Valid:  Epoch=114  ValidLoss=2400.97  ValAcc=0.919532\n",
      "\n",
      "Train:  Epoch=115  TrainLoss=79.5755351602  TrainAcc=0.989701948796\n",
      "Valid:  Epoch=115  ValidLoss=2498.26  ValAcc=0.915921\n",
      "\n",
      "Train:  Epoch=116  TrainLoss=72.3360123427  TrainAcc=0.988956820787\n",
      "Valid:  Epoch=116  ValidLoss=2444.36  ValAcc=0.916437\n",
      "\n",
      "Train:  Epoch=117  TrainLoss=40.3398187132  TrainAcc=0.994631257165\n",
      "Valid:  Epoch=117  ValidLoss=2362.34  ValAcc=0.92108\n",
      "\n",
      "Train:  Epoch=118  TrainLoss=24.3476232093  TrainAcc=0.995108903324\n",
      "Valid:  Epoch=118  ValidLoss=2196.13  ValAcc=0.925378\n",
      "\n",
      "Train:  Epoch=119  TrainLoss=24.4208570589  TrainAcc=0.995643867023\n",
      "Valid:  Epoch=119  ValidLoss=2098.33  ValAcc=0.928129\n",
      "\n",
      "Train:  Epoch=120  TrainLoss=98.8165943582  TrainAcc=0.992739778372\n",
      "Valid:  Epoch=120  ValidLoss=2454.82  ValAcc=0.923143\n",
      "\n",
      "Train:  Epoch=121  TrainLoss=74.2985197278  TrainAcc=0.99174627436\n",
      "Valid:  Epoch=121  ValidLoss=2704.12  ValAcc=0.914546\n",
      "\n",
      "Train:  Epoch=122  TrainLoss=27.6779204885  TrainAcc=0.994115399312\n",
      "Valid:  Epoch=122  ValidLoss=2207.51  ValAcc=0.923143\n",
      "\n",
      "Train:  Epoch=123  TrainLoss=35.2102992255  TrainAcc=0.995357279327\n",
      "Valid:  Epoch=123  ValidLoss=2293.26  ValAcc=0.924174\n",
      "\n",
      "Train:  Epoch=124  TrainLoss=33.254486298  TrainAcc=0.992873519297\n",
      "Valid:  Epoch=124  ValidLoss=2174.86  ValAcc=0.921423\n",
      "\n",
      "Train:  Epoch=125  TrainLoss=70.8513755411  TrainAcc=0.993255636225\n",
      "Valid:  Epoch=125  ValidLoss=2411.0  ValAcc=0.922971\n",
      "\n",
      "Train:  Epoch=126  TrainLoss=17.4065255061  TrainAcc=0.996446312572\n",
      "Valid:  Epoch=126  ValidLoss=2083.77  ValAcc=0.927613\n",
      "\n",
      "Train:  Epoch=127  TrainLoss=31.4505097126  TrainAcc=0.99359954146\n",
      "Valid:  Epoch=127  ValidLoss=2179.71  ValAcc=0.922799\n",
      "\n",
      "Train:  Epoch=128  TrainLoss=24.7197805061  TrainAcc=0.994382881162\n",
      "Valid:  Epoch=128  ValidLoss=2016.27  ValAcc=0.928301\n",
      "\n",
      "Train:  Epoch=129  TrainLoss=106.548280827  TrainAcc=0.989778372182\n",
      "Valid:  Epoch=129  ValidLoss=2522.29  ValAcc=0.917813\n",
      "\n",
      "Train:  Epoch=130  TrainLoss=11.766550079  TrainAcc=0.997420710737\n",
      "Valid:  Epoch=130  ValidLoss=2181.77  ValAcc=0.9326\n",
      "\n",
      "Train:  Epoch=131  TrainLoss=32.5893363249  TrainAcc=0.993446694689\n",
      "Valid:  Epoch=131  ValidLoss=2274.66  ValAcc=0.925894\n",
      "\n",
      "Train:  Epoch=132  TrainLoss=60.9115862732  TrainAcc=0.992758884219\n",
      "Valid:  Epoch=132  ValidLoss=2440.8  ValAcc=0.921423\n",
      "\n",
      "Train:  Epoch=133  TrainLoss=15.7396441993  TrainAcc=0.997382499045\n",
      "Valid:  Epoch=133  ValidLoss=2028.59  ValAcc=0.932256\n",
      "\n",
      "Train:  Epoch=134  TrainLoss=20.2642639909  TrainAcc=0.99566297287\n",
      "Valid:  Epoch=134  ValidLoss=2070.58  ValAcc=0.931912\n",
      "\n",
      "Train:  Epoch=135  TrainLoss=18.9013932599  TrainAcc=0.99635078334\n",
      "Valid:  Epoch=135  ValidLoss=2148.86  ValAcc=0.929848\n",
      "\n",
      "Train:  Epoch=136  TrainLoss=26.4207685017  TrainAcc=0.995471914406\n",
      "Valid:  Epoch=136  ValidLoss=2051.28  ValAcc=0.929161\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  Epoch=137  TrainLoss=18.7560641759  TrainAcc=0.996446312572\n",
      "Valid:  Epoch=137  ValidLoss=2131.75  ValAcc=0.930364\n",
      "\n",
      "Train:  Epoch=138  TrainLoss=19.9933675708  TrainAcc=0.996274359954\n",
      "Valid:  Epoch=138  ValidLoss=2144.92  ValAcc=0.931568\n",
      "\n",
      "Train:  Epoch=139  TrainLoss=19.6161100049  TrainAcc=0.995949560566\n",
      "Valid:  Epoch=139  ValidLoss=2094.61  ValAcc=0.93002\n",
      "\n",
      "Train:  Epoch=140  TrainLoss=26.608528368  TrainAcc=0.996369889186\n",
      "Valid:  Epoch=140  ValidLoss=2203.33  ValAcc=0.935007\n",
      "\n",
      "Train:  Epoch=141  TrainLoss=28.6122053106  TrainAcc=0.994745892243\n",
      "Valid:  Epoch=141  ValidLoss=2058.53  ValAcc=0.929161\n",
      "\n",
      "Train:  Epoch=142  TrainLoss=8.2012841162  TrainAcc=0.997955674436\n",
      "Valid:  Epoch=142  ValidLoss=1982.53  ValAcc=0.935522\n",
      "\n",
      "Train:  Epoch=143  TrainLoss=9.39895194993  TrainAcc=0.998280473825\n",
      "Valid:  Epoch=143  ValidLoss=2014.12  ValAcc=0.935351\n",
      "\n",
      "Train:  Epoch=144  TrainLoss=16.4573970924  TrainAcc=0.997286969813\n",
      "Valid:  Epoch=144  ValidLoss=2118.51  ValAcc=0.930364\n",
      "\n",
      "Train:  Epoch=145  TrainLoss=46.7945839229  TrainAcc=0.9949751624\n",
      "Valid:  Epoch=145  ValidLoss=2384.33  ValAcc=0.93002\n",
      "\n",
      "Train:  Epoch=146  TrainLoss=17.8790802206  TrainAcc=0.997172334734\n",
      "Valid:  Epoch=146  ValidLoss=2241.53  ValAcc=0.930536\n",
      "\n",
      "Train:  Epoch=147  TrainLoss=10.7791179591  TrainAcc=0.997841039358\n",
      "Valid:  Epoch=147  ValidLoss=2023.57  ValAcc=0.933631\n",
      "\n",
      "Train:  Epoch=148  TrainLoss=11.7197500219  TrainAcc=0.997057699656\n",
      "Valid:  Epoch=148  ValidLoss=1993.89  ValAcc=0.935522\n",
      "\n",
      "Train:  Epoch=149  TrainLoss=13.0435062556  TrainAcc=0.997286969813\n",
      "Valid:  Epoch=149  ValidLoss=1983.93  ValAcc=0.93621\n",
      "\n",
      "Train:  Epoch=150  TrainLoss=11.7946623076  TrainAcc=0.997439816584\n",
      "Valid:  Epoch=150  ValidLoss=2011.29  ValAcc=0.935522\n",
      "\n",
      "Train:  Epoch=151  TrainLoss=11.084924441  TrainAcc=0.997497134123\n",
      "Valid:  Epoch=151  ValidLoss=2036.47  ValAcc=0.931568\n",
      "\n",
      "Train:  Epoch=152  TrainLoss=13.841734006  TrainAcc=0.997821933512\n",
      "Valid:  Epoch=152  ValidLoss=1989.07  ValAcc=0.937586\n",
      "\n",
      "Train:  Epoch=153  TrainLoss=22.3633070698  TrainAcc=0.995949560566\n",
      "Valid:  Epoch=153  ValidLoss=2026.71  ValAcc=0.932943\n",
      "\n",
      "Train:  Epoch=154  TrainLoss=52.6773706173  TrainAcc=0.996064195644\n",
      "Valid:  Epoch=154  ValidLoss=1875.97  ValAcc=0.936554\n",
      "\n",
      "Train:  Epoch=155  TrainLoss=75.4437395194  TrainAcc=0.992032862056\n",
      "Valid:  Epoch=155  ValidLoss=2253.66  ValAcc=0.926066\n",
      "\n",
      "Train:  Epoch=156  TrainLoss=13.257184149  TrainAcc=0.997344287352\n",
      "Valid:  Epoch=156  ValidLoss=1971.16  ValAcc=0.937242\n",
      "\n",
      "Train:  Epoch=157  TrainLoss=16.1373693921  TrainAcc=0.997267863966\n",
      "Valid:  Epoch=157  ValidLoss=1871.17  ValAcc=0.938789\n",
      "\n",
      "Train:  Epoch=158  TrainLoss=15.0868078443  TrainAcc=0.997344287352\n",
      "Valid:  Epoch=158  ValidLoss=1959.62  ValAcc=0.936898\n",
      "\n",
      "Train:  Epoch=159  TrainLoss=31.085261892  TrainAcc=0.994879633168\n",
      "Valid:  Epoch=159  ValidLoss=2010.95  ValAcc=0.929848\n",
      "\n",
      "Train:  Epoch=160  TrainLoss=26.0684315022  TrainAcc=0.996102407337\n",
      "Valid:  Epoch=160  ValidLoss=2122.88  ValAcc=0.930536\n",
      "\n",
      "Train:  Epoch=161  TrainLoss=10.4242372894  TrainAcc=0.997974780283\n",
      "Valid:  Epoch=161  ValidLoss=1890.86  ValAcc=0.935866\n",
      "\n",
      "Train:  Epoch=162  TrainLoss=3.53881688393  TrainAcc=0.998796331677\n",
      "Valid:  Epoch=162  ValidLoss=1777.6  ValAcc=0.938273\n",
      "\n",
      "Train:  Epoch=163  TrainLoss=15.2277375574  TrainAcc=0.997783721819\n",
      "Valid:  Epoch=163  ValidLoss=1820.47  ValAcc=0.939649\n",
      "\n",
      "Train:  Epoch=164  TrainLoss=7.39808085204  TrainAcc=0.997974780283\n",
      "Valid:  Epoch=164  ValidLoss=1818.9  ValAcc=0.936554\n",
      "\n",
      "Train:  Epoch=165  TrainLoss=24.1644756115  TrainAcc=0.995548337791\n",
      "Valid:  Epoch=165  ValidLoss=1939.1  ValAcc=0.936554\n",
      "\n",
      "Train:  Epoch=166  TrainLoss=13.2752846813  TrainAcc=0.998165838747\n",
      "Valid:  Epoch=166  ValidLoss=1933.04  ValAcc=0.93793\n",
      "\n",
      "Train:  Epoch=167  TrainLoss=111.92831876  TrainAcc=0.993714176538\n",
      "Valid:  Epoch=167  ValidLoss=2118.25  ValAcc=0.928989\n",
      "\n",
      "Train:  Epoch=168  TrainLoss=10.2483980312  TrainAcc=0.998242262132\n",
      "Valid:  Epoch=168  ValidLoss=1854.72  ValAcc=0.940337\n",
      "\n",
      "Train:  Epoch=169  TrainLoss=13.0804838293  TrainAcc=0.997898356897\n",
      "Valid:  Epoch=169  ValidLoss=1788.82  ValAcc=0.937758\n",
      "\n",
      "Train:  Epoch=170  TrainLoss=35.7932995612  TrainAcc=0.995529231945\n",
      "Valid:  Epoch=170  ValidLoss=2045.31  ValAcc=0.933115\n",
      "\n",
      "Train:  Epoch=171  TrainLoss=15.4896550305  TrainAcc=0.997344287352\n",
      "Valid:  Epoch=171  ValidLoss=1979.16  ValAcc=0.935007\n",
      "\n",
      "Train:  Epoch=172  TrainLoss=30.5980703885  TrainAcc=0.996331677493\n",
      "Valid:  Epoch=172  ValidLoss=1930.52  ValAcc=0.940337\n",
      "\n",
      "Train:  Epoch=173  TrainLoss=12.8769252466  TrainAcc=0.997611769201\n",
      "Valid:  Epoch=173  ValidLoss=1657.51  ValAcc=0.939649\n",
      "\n",
      "Train:  Epoch=174  TrainLoss=14.553391726  TrainAcc=0.997993886129\n",
      "Valid:  Epoch=174  ValidLoss=1818.68  ValAcc=0.942572\n",
      "\n",
      "Train:  Epoch=175  TrainLoss=15.3677484411  TrainAcc=0.996904852885\n",
      "Valid:  Epoch=175  ValidLoss=1854.97  ValAcc=0.939993\n",
      "\n",
      "Train:  Epoch=176  TrainLoss=13.3579725765  TrainAcc=0.997363393198\n",
      "Valid:  Epoch=176  ValidLoss=1858.09  ValAcc=0.93793\n",
      "\n",
      "Train:  Epoch=177  TrainLoss=18.2316396551  TrainAcc=0.996427206725\n",
      "Valid:  Epoch=177  ValidLoss=1901.95  ValAcc=0.936726\n",
      "\n",
      "Train:  Epoch=178  TrainLoss=43.9097542488  TrainAcc=0.992988154375\n",
      "Valid:  Epoch=178  ValidLoss=2008.1  ValAcc=0.934147\n",
      "\n",
      "Train:  Epoch=179  TrainLoss=17.3076736888  TrainAcc=0.998108521207\n",
      "Valid:  Epoch=179  ValidLoss=1792.5  ValAcc=0.943947\n",
      "\n",
      "Train:  Epoch=180  TrainLoss=17.6333362336  TrainAcc=0.99724875812\n",
      "Valid:  Epoch=180  ValidLoss=1839.45  ValAcc=0.939133\n",
      "\n",
      "Train:  Epoch=181  TrainLoss=8.28072863511  TrainAcc=0.998452426442\n",
      "Valid:  Epoch=181  ValidLoss=1934.73  ValAcc=0.940337\n",
      "\n",
      "Train:  Epoch=182  TrainLoss=23.4868889868  TrainAcc=0.996962170424\n",
      "Valid:  Epoch=182  ValidLoss=1883.26  ValAcc=0.939649\n",
      "\n",
      "Train:  Epoch=183  TrainLoss=30.8545991369  TrainAcc=0.996293465801\n",
      "Valid:  Epoch=183  ValidLoss=2015.26  ValAcc=0.935522\n",
      "\n",
      "Train:  Epoch=184  TrainLoss=13.6637624472  TrainAcc=0.997821933512\n",
      "Valid:  Epoch=184  ValidLoss=1802.32  ValAcc=0.941884\n",
      "\n",
      "Train:  Epoch=185  TrainLoss=9.55876455945  TrainAcc=0.997993886129\n",
      "Valid:  Epoch=185  ValidLoss=1792.87  ValAcc=0.938102\n",
      "\n",
      "Train:  Epoch=186  TrainLoss=71.4383179259  TrainAcc=0.994764998089\n",
      "Valid:  Epoch=186  ValidLoss=2074.75  ValAcc=0.934491\n",
      "\n",
      "Train:  Epoch=187  TrainLoss=86.4642604322  TrainAcc=0.992472296523\n",
      "Valid:  Epoch=187  ValidLoss=2217.7  ValAcc=0.933975\n",
      "\n",
      "Train:  Epoch=188  TrainLoss=38.8065844648  TrainAcc=0.995624761177\n",
      "Valid:  Epoch=188  ValidLoss=1949.56  ValAcc=0.935866\n",
      "\n",
      "Train:  Epoch=189  TrainLoss=13.9773914576  TrainAcc=0.997478028277\n",
      "Valid:  Epoch=189  ValidLoss=1772.33  ValAcc=0.940681\n",
      "\n",
      "Train:  Epoch=190  TrainLoss=34.7614305048  TrainAcc=0.997420710737\n",
      "Valid:  Epoch=190  ValidLoss=1763.27  ValAcc=0.939821\n",
      "\n",
      "Train:  Epoch=191  TrainLoss=11.9709432435  TrainAcc=0.997153228888\n",
      "Valid:  Epoch=191  ValidLoss=1811.93  ValAcc=0.939993\n",
      "\n",
      "Train:  Epoch=192  TrainLoss=11.7828952396  TrainAcc=0.997401604891\n",
      "Valid:  Epoch=192  ValidLoss=1800.84  ValAcc=0.942228\n",
      "\n",
      "Train:  Epoch=193  TrainLoss=39.9444926185  TrainAcc=0.993561329767\n",
      "Valid:  Epoch=193  ValidLoss=1940.36  ValAcc=0.934835\n",
      "\n",
      "Train:  Epoch=194  TrainLoss=17.3589149758  TrainAcc=0.996981276271\n",
      "Valid:  Epoch=194  ValidLoss=1688.95  ValAcc=0.942056\n",
      "\n",
      "Train:  Epoch=195  TrainLoss=6.62038560972  TrainAcc=0.998490638135\n",
      "Valid:  Epoch=195  ValidLoss=1659.64  ValAcc=0.942916\n",
      "\n",
      "Train:  Epoch=196  TrainLoss=14.9855050027  TrainAcc=0.997497134123\n",
      "Valid:  Epoch=196  ValidLoss=1757.99  ValAcc=0.939993\n",
      "\n",
      "Train:  Epoch=197  TrainLoss=15.7509292541  TrainAcc=0.996981276271\n",
      "Valid:  Epoch=197  ValidLoss=1766.68  ValAcc=0.941024\n",
      "\n",
      "Train:  Epoch=198  TrainLoss=13.0948106753  TrainAcc=0.997764615972\n",
      "Valid:  Epoch=198  ValidLoss=1766.02  ValAcc=0.944979\n",
      "\n",
      "Train:  Epoch=199  TrainLoss=18.3087228814  TrainAcc=0.997172334734\n",
      "Valid:  Epoch=199  ValidLoss=1751.52  ValAcc=0.941025\n",
      "\n",
      "Train:  Epoch=200  TrainLoss=30.5801306156  TrainAcc=0.996465418418\n",
      "Valid:  Epoch=200  ValidLoss=1805.35  ValAcc=0.941368\n",
      "\n",
      "Train:  Epoch=201  TrainLoss=17.23857179  TrainAcc=0.996541841804\n",
      "Valid:  Epoch=201  ValidLoss=1623.48  ValAcc=0.945667\n",
      "\n",
      "Train:  Epoch=202  TrainLoss=45.3325740317  TrainAcc=0.994191822698\n",
      "Valid:  Epoch=202  ValidLoss=1892.1  ValAcc=0.935522\n",
      "\n",
      "Train:  Epoch=203  TrainLoss=15.5856455002  TrainAcc=0.997783721819\n",
      "Valid:  Epoch=203  ValidLoss=1722.83  ValAcc=0.942228\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  Epoch=204  TrainLoss=4.31507363704  TrainAcc=0.999254871991\n",
      "Valid:  Epoch=204  ValidLoss=1621.09  ValAcc=0.946355\n",
      "\n",
      "Train:  Epoch=205  TrainLoss=11.3679499026  TrainAcc=0.998528849828\n",
      "Valid:  Epoch=205  ValidLoss=1756.44  ValAcc=0.946698\n",
      "\n",
      "Train:  Epoch=206  TrainLoss=10.104524882  TrainAcc=0.99772640428\n",
      "Valid:  Epoch=206  ValidLoss=1651.0  ValAcc=0.946698\n",
      "\n",
      "Train:  Epoch=207  TrainLoss=43.5950984236  TrainAcc=0.992625143294\n",
      "Valid:  Epoch=207  ValidLoss=1808.7  ValAcc=0.939305\n",
      "\n",
      "Train:  Epoch=208  TrainLoss=10.0095893627  TrainAcc=0.998242262132\n",
      "Valid:  Epoch=208  ValidLoss=1723.43  ValAcc=0.944119\n",
      "\n",
      "Train:  Epoch=209  TrainLoss=5.89150154624  TrainAcc=0.998605273214\n",
      "Valid:  Epoch=209  ValidLoss=1727.3  ValAcc=0.945667\n",
      "\n",
      "Train:  Epoch=210  TrainLoss=16.541485274  TrainAcc=0.996446312572\n",
      "Valid:  Epoch=210  ValidLoss=1684.21  ValAcc=0.943604\n",
      "\n",
      "Train:  Epoch=211  TrainLoss=27.2718937741  TrainAcc=0.99793656859\n",
      "Valid:  Epoch=211  ValidLoss=1773.92  ValAcc=0.945151\n",
      "\n",
      "Train:  Epoch=212  TrainLoss=8.87532604665  TrainAcc=0.998739014138\n",
      "Valid:  Epoch=212  ValidLoss=1653.41  ValAcc=0.946527\n",
      "\n",
      "Train:  Epoch=213  TrainLoss=12.6491896758  TrainAcc=0.997841039358\n",
      "Valid:  Epoch=213  ValidLoss=1660.94  ValAcc=0.946011\n",
      "\n",
      "Train:  Epoch=214  TrainLoss=14.4326961704  TrainAcc=0.997191440581\n",
      "Valid:  Epoch=214  ValidLoss=1667.88  ValAcc=0.947386\n",
      "\n",
      "Train:  Epoch=215  TrainLoss=6.1267922934  TrainAcc=0.999273977837\n",
      "Valid:  Epoch=215  ValidLoss=1546.91  ValAcc=0.950309\n",
      "\n",
      "Train:  Epoch=216  TrainLoss=9.32585689704  TrainAcc=0.998012991976\n",
      "Valid:  Epoch=216  ValidLoss=1512.35  ValAcc=0.946355\n",
      "\n",
      "Train:  Epoch=217  TrainLoss=7.70951041541  TrainAcc=0.998376003057\n",
      "Valid:  Epoch=217  ValidLoss=1550.11  ValAcc=0.947042\n",
      "\n",
      "Train:  Epoch=218  TrainLoss=17.0846153208  TrainAcc=0.997535345816\n",
      "Valid:  Epoch=218  ValidLoss=1607.17  ValAcc=0.944291\n",
      "\n",
      "Train:  Epoch=219  TrainLoss=10.9760920831  TrainAcc=0.998204050439\n",
      "Valid:  Epoch=219  ValidLoss=1705.35  ValAcc=0.946183\n",
      "\n",
      "Train:  Epoch=220  TrainLoss=6.97133286702  TrainAcc=0.998662590753\n",
      "Valid:  Epoch=220  ValidLoss=1465.64  ValAcc=0.94773\n",
      "\n",
      "Train:  Epoch=221  TrainLoss=24.2644163431  TrainAcc=0.997630875048\n",
      "Valid:  Epoch=221  ValidLoss=1670.78  ValAcc=0.945323\n",
      "\n",
      "Train:  Epoch=222  TrainLoss=8.51788177071  TrainAcc=0.998376003057\n",
      "Valid:  Epoch=222  ValidLoss=1518.1  ValAcc=0.94945\n",
      "\n",
      "Train:  Epoch=223  TrainLoss=13.8224890036  TrainAcc=0.998471532289\n",
      "Valid:  Epoch=223  ValidLoss=1600.43  ValAcc=0.947042\n",
      "\n",
      "Train:  Epoch=224  TrainLoss=12.7226239447  TrainAcc=0.997286969813\n",
      "Valid:  Epoch=224  ValidLoss=1531.16  ValAcc=0.94859\n",
      "\n",
      "Train:  Epoch=225  TrainLoss=7.77266657472  TrainAcc=0.998777225831\n",
      "Valid:  Epoch=225  ValidLoss=1626.87  ValAcc=0.948074\n",
      "\n",
      "Train:  Epoch=226  TrainLoss=6.88084214077  TrainAcc=0.99862437906\n",
      "Valid:  Epoch=226  ValidLoss=1509.94  ValAcc=0.947558\n",
      "\n",
      "Train:  Epoch=227  TrainLoss=4.88249860294  TrainAcc=0.999273977837\n",
      "Valid:  Epoch=227  ValidLoss=1491.85  ValAcc=0.949793\n",
      "\n",
      "Train:  Epoch=228  TrainLoss=9.51548142522  TrainAcc=0.998509743982\n",
      "Valid:  Epoch=228  ValidLoss=1561.32  ValAcc=0.951341\n",
      "\n",
      "Train:  Epoch=229  TrainLoss=15.1948371272  TrainAcc=0.998681696599\n",
      "Valid:  Epoch=229  ValidLoss=1469.76  ValAcc=0.952029\n",
      "\n",
      "Train:  Epoch=230  TrainLoss=62.8873373919  TrainAcc=0.99608330149\n",
      "Valid:  Epoch=230  ValidLoss=1717.89  ValAcc=0.94326\n",
      "\n",
      "Train:  Epoch=231  TrainLoss=7.79737374033  TrainAcc=0.998452426442\n",
      "Valid:  Epoch=231  ValidLoss=1554.9  ValAcc=0.952201\n",
      "\n",
      "Train:  Epoch=232  TrainLoss=12.2280181181  TrainAcc=0.998299579671\n",
      "Valid:  Epoch=232  ValidLoss=1545.89  ValAcc=0.948934\n",
      "\n",
      "Train:  Epoch=233  TrainLoss=2.76294650441  TrainAcc=0.999350401223\n",
      "Valid:  Epoch=233  ValidLoss=1513.08  ValAcc=0.950137\n",
      "\n",
      "Train:  Epoch=234  TrainLoss=9.13194557324  TrainAcc=0.998395108903\n",
      "Valid:  Epoch=234  ValidLoss=1536.81  ValAcc=0.949106\n",
      "\n",
      "Train:  Epoch=235  TrainLoss=6.68526158717  TrainAcc=0.998719908292\n",
      "Valid:  Epoch=235  ValidLoss=1633.02  ValAcc=0.948074\n",
      "\n",
      "Train:  Epoch=236  TrainLoss=8.72371329193  TrainAcc=0.998452426442\n",
      "Valid:  Epoch=236  ValidLoss=1607.24  ValAcc=0.94859\n",
      "\n",
      "Train:  Epoch=237  TrainLoss=17.4248873651  TrainAcc=0.997630875048\n",
      "Valid:  Epoch=237  ValidLoss=1571.05  ValAcc=0.94773\n",
      "\n",
      "Train:  Epoch=238  TrainLoss=3.71227456918  TrainAcc=0.999044707681\n",
      "Valid:  Epoch=238  ValidLoss=1441.06  ValAcc=0.950653\n",
      "\n",
      "Train:  Epoch=239  TrainLoss=8.31156057978  TrainAcc=0.998280473825\n",
      "Valid:  Epoch=239  ValidLoss=1523.73  ValAcc=0.947902\n",
      "\n",
      "Train:  Epoch=240  TrainLoss=6.56060847077  TrainAcc=0.998853649217\n",
      "Valid:  Epoch=240  ValidLoss=1579.97  ValAcc=0.949965\n",
      "\n",
      "Train:  Epoch=241  TrainLoss=14.5160510263  TrainAcc=0.997688192587\n",
      "Valid:  Epoch=241  ValidLoss=1597.64  ValAcc=0.950137\n",
      "\n",
      "Train:  Epoch=242  TrainLoss=24.0158139758  TrainAcc=0.995605655331\n",
      "Valid:  Epoch=242  ValidLoss=1478.35  ValAcc=0.944807\n",
      "\n",
      "Train:  Epoch=243  TrainLoss=1.53394391578  TrainAcc=0.999407718762\n",
      "Valid:  Epoch=243  ValidLoss=1448.7  ValAcc=0.951341\n",
      "\n",
      "Train:  Epoch=244  TrainLoss=3.87636644367  TrainAcc=0.998891860909\n",
      "Valid:  Epoch=244  ValidLoss=1477.77  ValAcc=0.950137\n",
      "\n",
      "Train:  Epoch=245  TrainLoss=15.541897699  TrainAcc=0.997821933512\n",
      "Valid:  Epoch=245  ValidLoss=1541.42  ValAcc=0.946699\n",
      "\n",
      "Train:  Epoch=246  TrainLoss=11.2593018564  TrainAcc=0.998356897211\n",
      "Valid:  Epoch=246  ValidLoss=1456.51  ValAcc=0.951169\n",
      "\n",
      "Train:  Epoch=247  TrainLoss=20.4728451124  TrainAcc=0.996904852885\n",
      "Valid:  Epoch=247  ValidLoss=1526.2  ValAcc=0.947214\n",
      "\n",
      "Train:  Epoch=248  TrainLoss=11.3799332593  TrainAcc=0.997879251051\n",
      "Valid:  Epoch=248  ValidLoss=1575.13  ValAcc=0.944979\n",
      "\n",
      "Train:  Epoch=249  TrainLoss=18.5522388089  TrainAcc=0.996828429499\n",
      "Valid:  Epoch=249  ValidLoss=1535.95  ValAcc=0.945323\n",
      "\n",
      "Train:  Epoch=250  TrainLoss=1.78198680775  TrainAcc=0.999656094765\n",
      "Valid:  Epoch=250  ValidLoss=1512.74  ValAcc=0.94945\n",
      "\n",
      "Train:  Epoch=251  TrainLoss=5.57021327028  TrainAcc=0.999006495988\n",
      "Valid:  Epoch=251  ValidLoss=1464.16  ValAcc=0.950481\n",
      "\n",
      "Train:  Epoch=252  TrainLoss=39.3202426965  TrainAcc=0.998662590753\n",
      "Valid:  Epoch=252  ValidLoss=1620.91  ValAcc=0.950653\n",
      "\n",
      "Train:  Epoch=253  TrainLoss=2.61885507296  TrainAcc=0.999254871991\n",
      "Valid:  Epoch=253  ValidLoss=1397.45  ValAcc=0.952201\n",
      "\n",
      "Train:  Epoch=254  TrainLoss=3.45567036546  TrainAcc=0.999465036301\n",
      "Valid:  Epoch=254  ValidLoss=1358.55  ValAcc=0.953748\n",
      "\n",
      "Train:  Epoch=255  TrainLoss=5.61635473188  TrainAcc=0.999121131066\n",
      "Valid:  Epoch=255  ValidLoss=1439.1  ValAcc=0.952029\n",
      "\n",
      "Train:  Epoch=256  TrainLoss=2.6860976201  TrainAcc=0.999541459687\n",
      "Valid:  Epoch=256  ValidLoss=1346.25  ValAcc=0.956327\n",
      "\n",
      "Train:  Epoch=257  TrainLoss=20.5202746427  TrainAcc=0.997076805502\n",
      "Valid:  Epoch=257  ValidLoss=1661.75  ValAcc=0.94687\n",
      "\n",
      "Train:  Epoch=258  TrainLoss=8.99743640679  TrainAcc=0.998567061521\n",
      "Valid:  Epoch=258  ValidLoss=1343.47  ValAcc=0.952372\n",
      "\n",
      "Train:  Epoch=259  TrainLoss=4.65094601908  TrainAcc=0.999082919373\n",
      "Valid:  Epoch=259  ValidLoss=1296.68  ValAcc=0.95392\n",
      "\n",
      "Train:  Epoch=260  TrainLoss=16.1981826979  TrainAcc=0.997382499045\n",
      "Valid:  Epoch=260  ValidLoss=1565.82  ValAcc=0.947902\n",
      "\n",
      "Train:  Epoch=261  TrainLoss=6.78297513369  TrainAcc=0.999140236912\n",
      "Valid:  Epoch=261  ValidLoss=1294.56  ValAcc=0.955467\n",
      "\n",
      "Train:  Epoch=262  TrainLoss=16.1955815644  TrainAcc=0.997497134123\n",
      "Valid:  Epoch=262  ValidLoss=1382.69  ValAcc=0.952029\n",
      "\n",
      "Train:  Epoch=263  TrainLoss=10.7393837381  TrainAcc=0.997917462744\n",
      "Valid:  Epoch=263  ValidLoss=1371.22  ValAcc=0.954092\n",
      "\n",
      "Train:  Epoch=264  TrainLoss=21.4870321991  TrainAcc=0.998012991976\n",
      "Valid:  Epoch=264  ValidLoss=1485.1  ValAcc=0.949793\n",
      "\n",
      "Train:  Epoch=265  TrainLoss=3.04235532358  TrainAcc=0.999235766144\n",
      "Valid:  Epoch=265  ValidLoss=1333.37  ValAcc=0.954092\n",
      "\n",
      "Train:  Epoch=266  TrainLoss=26.3783616425  TrainAcc=0.998223156286\n",
      "Valid:  Epoch=266  ValidLoss=1396.03  ValAcc=0.952029\n",
      "\n",
      "Train:  Epoch=267  TrainLoss=4.78624954436  TrainAcc=0.999025601834\n",
      "Valid:  Epoch=267  ValidLoss=1431.57  ValAcc=0.953748\n",
      "\n",
      "Train:  Epoch=268  TrainLoss=7.07760700995  TrainAcc=0.998471532289\n",
      "Valid:  Epoch=268  ValidLoss=1403.64  ValAcc=0.949793\n",
      "\n",
      "Train:  Epoch=269  TrainLoss=13.5925357586  TrainAcc=0.998223156286\n",
      "Valid:  Epoch=269  ValidLoss=1469.37  ValAcc=0.951513\n",
      "\n",
      "Train:  Epoch=270  TrainLoss=4.22704787043  TrainAcc=0.999140236912\n",
      "Valid:  Epoch=270  ValidLoss=1429.95  ValAcc=0.950653\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  Epoch=271  TrainLoss=9.97573229083  TrainAcc=0.998509743982\n",
      "Valid:  Epoch=271  ValidLoss=1529.15  ValAcc=0.949621\n",
      "\n",
      "Train:  Epoch=272  TrainLoss=2.67411523868  TrainAcc=0.999579671379\n",
      "Valid:  Epoch=272  ValidLoss=1343.22  ValAcc=0.954092\n",
      "\n",
      "Train:  Epoch=273  TrainLoss=0.433727453218  TrainAcc=0.999732518151\n",
      "Valid:  Epoch=273  ValidLoss=1363.72  ValAcc=0.953576\n",
      "\n",
      "Train:  Epoch=274  TrainLoss=1.36382176267  TrainAcc=0.999656094765\n",
      "Valid:  Epoch=274  ValidLoss=1451.68  ValAcc=0.951169\n",
      "\n",
      "Train:  Epoch=275  TrainLoss=15.7680396378  TrainAcc=0.998223156286\n",
      "Valid:  Epoch=275  ValidLoss=1587.34  ValAcc=0.948246\n",
      "\n",
      "Train:  Epoch=276  TrainLoss=23.5586814065  TrainAcc=0.998070309515\n",
      "Valid:  Epoch=276  ValidLoss=1608.36  ValAcc=0.950825\n",
      "\n",
      "Train:  Epoch=277  TrainLoss=5.28653473021  TrainAcc=0.998700802446\n",
      "Valid:  Epoch=277  ValidLoss=1409.08  ValAcc=0.954092\n",
      "\n",
      "Train:  Epoch=278  TrainLoss=4.03896090858  TrainAcc=0.998891860909\n",
      "Valid:  Epoch=278  ValidLoss=1381.37  ValAcc=0.954952\n",
      "\n",
      "Train:  Epoch=279  TrainLoss=13.9114938315  TrainAcc=0.997688192587\n",
      "Valid:  Epoch=279  ValidLoss=1478.79  ValAcc=0.952201\n",
      "\n",
      "Train:  Epoch=280  TrainLoss=19.5460470998  TrainAcc=0.996675582728\n",
      "Valid:  Epoch=280  ValidLoss=1527.24  ValAcc=0.94859\n",
      "\n",
      "Train:  Epoch=281  TrainLoss=32.9103022475  TrainAcc=0.996484524264\n",
      "Valid:  Epoch=281  ValidLoss=1420.6  ValAcc=0.949793\n",
      "\n",
      "Train:  Epoch=282  TrainLoss=9.60071279863  TrainAcc=0.998643484906\n",
      "Valid:  Epoch=282  ValidLoss=1373.48  ValAcc=0.955811\n",
      "\n",
      "Train:  Epoch=283  TrainLoss=9.92291281718  TrainAcc=0.997554451662\n",
      "Valid:  Epoch=283  ValidLoss=1311.73  ValAcc=0.954092\n",
      "\n",
      "Train:  Epoch=284  TrainLoss=2.06905832004  TrainAcc=0.999598777226\n",
      "Valid:  Epoch=284  ValidLoss=1299.67  ValAcc=0.956843\n",
      "\n",
      "Train:  Epoch=285  TrainLoss=5.68875258895  TrainAcc=0.999063813527\n",
      "Valid:  Epoch=285  ValidLoss=1382.7  ValAcc=0.955639\n",
      "\n",
      "Train:  Epoch=286  TrainLoss=19.2402974189  TrainAcc=0.998127627054\n",
      "Valid:  Epoch=286  ValidLoss=1411.79  ValAcc=0.954608\n",
      "\n",
      "Train:  Epoch=287  TrainLoss=13.7048881325  TrainAcc=0.996752006114\n",
      "Valid:  Epoch=287  ValidLoss=1363.77  ValAcc=0.950997\n",
      "\n",
      "Train:  Epoch=288  TrainLoss=17.3774524832  TrainAcc=0.997286969813\n",
      "Valid:  Epoch=288  ValidLoss=1484.16  ValAcc=0.950825\n",
      "\n",
      "Train:  Epoch=289  TrainLoss=3.01233893543  TrainAcc=0.999484142147\n",
      "Valid:  Epoch=289  ValidLoss=1328.35  ValAcc=0.957359\n",
      "\n",
      "Train:  Epoch=290  TrainLoss=7.24360239166  TrainAcc=0.998853649217\n",
      "Valid:  Epoch=290  ValidLoss=1500.23  ValAcc=0.952716\n",
      "\n",
      "Train:  Epoch=291  TrainLoss=9.97222027573  TrainAcc=0.998356897211\n",
      "Valid:  Epoch=291  ValidLoss=1436.78  ValAcc=0.949278\n",
      "\n",
      "Train:  Epoch=292  TrainLoss=9.67972476669  TrainAcc=0.9981467329\n",
      "Valid:  Epoch=292  ValidLoss=1295.4  ValAcc=0.956155\n",
      "\n",
      "Train:  Epoch=293  TrainLoss=2.34694598197  TrainAcc=0.999541459687\n",
      "Valid:  Epoch=293  ValidLoss=1314.93  ValAcc=0.956327\n",
      "\n",
      "Train:  Epoch=294  TrainLoss=1.35851808046  TrainAcc=0.999713412304\n",
      "Valid:  Epoch=294  ValidLoss=1356.86  ValAcc=0.956155\n",
      "\n",
      "Train:  Epoch=295  TrainLoss=1.51559305468  TrainAcc=0.999503247994\n",
      "Valid:  Epoch=295  ValidLoss=1415.63  ValAcc=0.953404\n",
      "\n",
      "Train:  Epoch=296  TrainLoss=1.26172183012  TrainAcc=0.999636988919\n",
      "Valid:  Epoch=296  ValidLoss=1343.66  ValAcc=0.957187\n",
      "\n",
      "Train:  Epoch=297  TrainLoss=53.6597577854  TrainAcc=0.995128009171\n",
      "Valid:  Epoch=297  ValidLoss=1808.52  ValAcc=0.946698\n",
      "\n",
      "Train:  Epoch=298  TrainLoss=1.57841241869  TrainAcc=0.999732518151\n",
      "Valid:  Epoch=298  ValidLoss=1404.86  ValAcc=0.954436\n",
      "\n",
      "Train:  Epoch=299  TrainLoss=1.8397197386  TrainAcc=0.999656094765\n",
      "Valid:  Epoch=299  ValidLoss=1392.88  ValAcc=0.955124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training network\n",
    "e = 0\n",
    "while(e < epochs):\n",
    "\t# batch pointer\n",
    "\tbatch_start = 0\n",
    "\tbatch_end = batch_size\n",
    "\t# training in batches\n",
    "\twhile(batch_end <= len(X_train)):\n",
    "\t\t# making batch\n",
    "\t\tX_batch = X_train[batch_start : batch_end]\n",
    "\t\tY_batch = Y_train_one_hot[batch_start : batch_end]\n",
    "\t\t# training step\n",
    "\t\tsess.run(optimizer, feed_dict={X: X_batch, Y: np.array(Y_batch), keep_prob: keep_probab})\n",
    "\t\t# updating batch pointers\n",
    "\t\tbatch_start = batch_end\n",
    "\t\tbatch_end = batch_end + batch_size\n",
    "\t\tif(batch_start < len(X_train) and batch_end > len(X_train)):\n",
    "\t\t\tbatch_end = len(X_train)\n",
    "\t# batch pointer\n",
    "\tbatch_start = 0\n",
    "\tbatch_end = batch_size\n",
    "\t# finding train accuracy in batches\n",
    "\teval_batch_size = len(X_val)\n",
    "\ttrain_loss = 0\n",
    "\ttrain_matches = 0\n",
    "\twhile(batch_end <= len(X_train)):\n",
    "\t\t# making batch\n",
    "\t\tX_batch = X_train[batch_start : batch_end]\n",
    "\t\tY_batch = Y_train_one_hot[batch_start : batch_end]\n",
    "\t\t# getting train accuracy\n",
    "\t\tloss, mat = sess.run([cost, matches], feed_dict={X: X_batch, Y: np.array(Y_batch), keep_prob: keep_probab})\n",
    "\t\ttrain_loss = train_loss + loss\n",
    "\t\tif(batch_start == 0):\n",
    "\t\t\ttrain_matches = mat\n",
    "\t\telse:\n",
    "\t\t\ttrain_matches = np.concatenate((train_matches, mat))\n",
    "\t\t# updating batch pointers\n",
    "\t\tbatch_start = batch_end\n",
    "\t\tbatch_end = batch_end + eval_batch_size\n",
    "\t\tif(batch_start < len(X_train) and batch_end > len(X_train)):\n",
    "\t\t\tbatch_end = len(X_train)\n",
    "\t# calculating training loss and training accuracy\n",
    "\ttrain_loss = train_loss / float(len(X_train)/float(eval_batch_size))\n",
    "\ttrain_acc = np.mean(train_matches)\n",
    "\tprint(\"Train:  Epoch=\"+str(e)+\"  TrainLoss=\"+str(train_loss)+\"  TrainAcc=\"+str(train_acc))\n",
    "\t# getting validation accuracy\n",
    "\tloss, acc = sess.run([cost, accuracy], feed_dict={X: X_val, Y: np.array(Y_val_one_hot), keep_prob: keep_probab})\n",
    "\tprint(\"Valid:  Epoch=\"+str(e)+\"  ValidLoss=\"+str(loss)+\"  ValAcc=\"+str(acc))\n",
    "\tprint\n",
    "\t# updating epoch\n",
    "\te = e + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading test data, 5 images\n",
    "test_data = np.load(test_data_path)\n",
    "test_label = np.load(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data: (5, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# normalizing test data\n",
    "test_data = scaler.transform(test_data)\n",
    "test_data = np.reshape(test_data, (len(test_data), 32, 32, 3))\n",
    "print(\"Shape of test data: \" + str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# getting test accuracy\n",
    "test_pred = sess.run(pred_step, feed_dict = {X: test_data, keep_prob: keep_probab})\n",
    "test_accuracy = accuracy_score(test_pred, test_label)\n",
    "print(\"Test Accuracy: \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopKV2(values=array([[ 263059.5     ,  177488.328125,  153980.90625 ,  151166.671875,\n",
      "         115443.15625 ],\n",
      "       [ 272456.      ,  227229.109375,  207690.625   ,  183878.3125  ,\n",
      "         157665.484375],\n",
      "       [ 188688.578125,  177094.875   ,  171138.9375  ,  137037.859375,\n",
      "         135236.734375],\n",
      "       [ 307451.1875  ,  293140.375   ,  264120.125   ,  251648.359375,\n",
      "         181204.46875 ],\n",
      "       [ 385609.8125  ,  283905.375   ,  186123.640625,  166877.328125,\n",
      "         162580.859375]], dtype=float32), indices=array([[36, 13, 25, 42, 37],\n",
      "       [35, 13, 40, 10, 38],\n",
      "       [10, 25, 40,  1, 18],\n",
      "       [25,  0, 13, 16, 31],\n",
      "       [37, 25, 39,  0, 23]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# getting top 5 predicted classes for test images\n",
    "test_pred_prob = sess.run(out, feed_dict={X: test_data, keep_prob: keep_probab})\n",
    "top_k = sess.run(tf.nn.top_k(test_pred_prob, k=5, sorted=True))\n",
    "print(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
